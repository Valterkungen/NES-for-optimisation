{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m\n\u001b[1;32m    196\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Run all configurations and generate plots\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mevaluate_nn_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfigurations\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    201\u001b[0m plot_comparisons(results)\n",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Run all configurations and generate plots\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mevaluate_nn_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configurations]\n\u001b[1;32m    201\u001b[0m plot_comparisons(results)\n",
      "Cell \u001b[0;32mIn[1], line 111\u001b[0m, in \u001b[0;36mevaluate_nn_configuration\u001b[0;34m(layer_sizes, population_size, generations)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mevaluate_loss(X, y)\n\u001b[1;32m    110\u001b[0m nes \u001b[38;5;241m=\u001b[39m NaturalEvolutionStrategy(network, population_size, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, sigma_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mnes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m grid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m    114\u001b[0m x1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, grid_size)\n",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m, in \u001b[0;36mNaturalEvolutionStrategy.optimize\u001b[0;34m(self, fitness_function, generations, sigma_min, sigma_max)\u001b[0m\n\u001b[1;32m     39\u001b[0m fisher \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(log_derivative_mean\u001b[38;5;241m.\u001b[39mT, log_derivative_mean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size\n\u001b[1;32m     40\u001b[0m reg_fisher \u001b[38;5;241m=\u001b[39m fisher \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meye(fisher\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg_fisher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_mean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m grad_sigma)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma, sigma_min, sigma_max)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/numpy/linalg/linalg.py:409\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    407\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 409\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import time\n",
    "\n",
    "# Natural Evolution Strategies (NES) Implementation\n",
    "class NaturalEvolutionStrategy:\n",
    "    def __init__(self, network, population_size, learning_rate, sigma_init, seed=None):\n",
    "        self.network = network\n",
    "        self.population_size = population_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma_init\n",
    "        self.mean = self.network.get_parameters()\n",
    "        self.dim = len(self.mean)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def fitness_shaping(self, rewards):\n",
    "        ranks = np.argsort(np.argsort(-rewards))\n",
    "        utilities = np.maximum(0, np.log(self.population_size / 2 + 1) - np.log(ranks + 1))\n",
    "        utilities -= np.mean(utilities)\n",
    "        return utilities\n",
    "\n",
    "    def optimize(self, fitness_function, generations, sigma_min=1e-5, sigma_max=10.0):\n",
    "        self.loss_history = []\n",
    "        self.time_history = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for gen in range(generations):\n",
    "            samples = self.rng.normal(self.mean, self.sigma, (self.population_size, self.dim))\n",
    "            fitness = np.array([fitness_function(sample) for sample in samples])\n",
    "            utilities = self.fitness_shaping(fitness)\n",
    "\n",
    "            log_derivative_mean = (samples - self.mean) / self.sigma**2\n",
    "            log_derivative_sigma = (np.linalg.norm(samples - self.mean, axis=1)**2 - self.dim) / self.sigma\n",
    "            grad_mean = np.dot(utilities, log_derivative_mean) / self.population_size\n",
    "            grad_sigma = np.dot(utilities, log_derivative_sigma) / self.population_size\n",
    "\n",
    "            fisher = np.dot(log_derivative_mean.T, log_derivative_mean) / self.population_size\n",
    "            reg_fisher = fisher + np.eye(fisher.shape[0]) * 1e-5\n",
    "\n",
    "            self.mean += self.learning_rate * np.linalg.solve(reg_fisher, grad_mean)\n",
    "            self.sigma *= np.exp(self.learning_rate / 2 * grad_sigma)\n",
    "            self.sigma = np.clip(self.sigma, sigma_min, sigma_max)\n",
    "\n",
    "            self.loss_history.append(-np.mean(fitness))\n",
    "            self.time_history.append(time.time() - start_time)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.mean\n",
    "\n",
    "# Neural Network with Support for Multiple Layers\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.params = self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        params = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            input_size, output_size = self.layer_sizes[i], self.layer_sizes[i + 1]\n",
    "            params.append(np.random.randn(input_size * output_size) * 0.1)\n",
    "            params.append(np.random.randn(output_size) * 0.1)\n",
    "        return np.concatenate(params)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.params\n",
    "\n",
    "    def set_parameters(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def predict(self, X):\n",
    "        params = self.params\n",
    "        start = 0\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            input_size, output_size = self.layer_sizes[i], self.layer_sizes[i + 1]\n",
    "            end_weights = start + input_size * output_size\n",
    "            end_biases = end_weights + output_size\n",
    "\n",
    "            weights = params[start:end_weights].reshape(input_size, output_size)\n",
    "            biases = params[end_weights:end_biases]\n",
    "\n",
    "            X = np.dot(X, weights) + biases\n",
    "            if i < len(self.layer_sizes) - 2:\n",
    "                X = np.tanh(X)\n",
    "            start = end_biases\n",
    "        return X\n",
    "\n",
    "    def evaluate_loss(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean((predictions - y) ** 2)\n",
    "\n",
    "# Benchmarking Configurations\n",
    "def evaluate_nn_configuration(layer_sizes, population_size, generations=10000):\n",
    "    X = np.random.uniform(-1, 1, (100, 2))\n",
    "    y = np.sin(np.pi * X[:, 0]) + np.cos(np.pi * X[:, 1])\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    network = NeuralNetwork(layer_sizes=layer_sizes)\n",
    "\n",
    "    def fitness_function(params):\n",
    "        network.set_parameters(params)\n",
    "        return -network.evaluate_loss(X, y)\n",
    "\n",
    "    nes = NaturalEvolutionStrategy(network, population_size, learning_rate=0.1, sigma_init=0.5, seed=42)\n",
    "    nes.optimize(fitness_function, generations=generations)\n",
    "\n",
    "    grid_size = 50\n",
    "    x1 = np.linspace(-1, 1, grid_size)\n",
    "    x2 = np.linspace(-1, 1, grid_size)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    y_true = np.sin(np.pi * X_grid[:, 0]) + np.cos(np.pi * X_grid[:, 1])\n",
    "    y_pred = network.predict(X_grid).flatten()\n",
    "    errors = np.abs(y_true - y_pred).reshape(grid_size, grid_size)\n",
    "\n",
    "    return {\n",
    "        \"loss_history\": nes.loss_history,\n",
    "        \"time_history\": nes.time_history,\n",
    "        \"total_time\": nes.time_history[-1],  # Total time for all generations\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"errors\": errors,\n",
    "        \"configuration\": f\"Layers {layer_sizes}, Pop {population_size}\",\n",
    "        \"X_grid\": X_grid\n",
    "    }\n",
    "\n",
    "\n",
    "# Configurations to Compare\n",
    "configurations = [\n",
    "    {\"layer_sizes\": [2, 10, 1], \"population_size\": 50},\n",
    "    {\"layer_sizes\": [2, 10, 1], \"population_size\": 500},\n",
    "    {\"layer_sizes\": [2, 10, 20, 10, 1], \"population_size\": 50},\n",
    "    {\"layer_sizes\": [2, 10, 20, 10, 1], \"population_size\": 500},\n",
    "]\n",
    "\n",
    "\n",
    "# Plotting Results with Improvements\n",
    "def plot_comparisons(results):\n",
    "    # Plot MSE vs Generations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for result in results:\n",
    "        plt.plot(range(1, len(result[\"loss_history\"]) + 1), result[\"loss_history\"],\n",
    "                 label=f\"{result['configuration']} (Total Time: {result['total_time']:.2f}s)\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Generations (Log Scale)\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"MSE Loss vs Generations for Different Configurations\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a single 2x2 grid for \"True vs Predicted Output\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        X_grid = result[\"X_grid\"]\n",
    "        y_true = result[\"y_true\"]\n",
    "        y_pred = result[\"y_pred\"]\n",
    "\n",
    "        ax.scatter(X_grid[:, 0], y_true, alpha=0.5, label=\"True Function (X2 slices)\")\n",
    "        ax.scatter(X_grid[:, 0], y_pred, alpha=0.5, label=\"NN Predictions (X2 slices)\")\n",
    "        ax.set_title(result[\"configuration\"])\n",
    "        ax.set_xlabel(\"Input Feature (X1)\")\n",
    "        ax.set_ylabel(\"Output (y)\")\n",
    "        ax.legend()\n",
    "\n",
    "    fig.suptitle(\"True vs Predicted Output for All Configurations\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a single 2x2 grid for heatmaps with shared color bar\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    all_errors = [result[\"errors\"] for result in results]\n",
    "    min_error, max_error = np.min(all_errors), np.max(all_errors  # Get global min/max for color normalization\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        im = ax.imshow(result[\"errors\"], extent=(-1, 1, -1, 1), origin=\"lower\", cmap=\"plasma\",\n",
    "                       vmin=min_error, vmax=max_error, aspect=\"auto\")\n",
    "        ax.set_title(result[\"configuration\"])\n",
    "        ax.set_xlabel(\"X1\")\n",
    "        ax.set_ylabel(\"X2\")\n",
    "\n",
    "    # Add a single color bar for all subplots\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    fig.colorbar(im, cax=cbar_ax, label=\"Absolute Error\")\n",
    "\n",
    "    fig.suptitle(\"Prediction Errors Heatmap for All Configurations\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout to accommodate color bar\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run all configurations and generate plots\n",
    "results = [evaluate_nn_configuration(**config) for config in configurations]\n",
    "plot_comparisons(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
